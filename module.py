# –≠—Ç–æ—Ç –º–æ–¥—É–ª—å –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —à—Ç—É–∫–∏
# ‚Ä¢ This module handles all other stuff
# –ù–∞–ø—Ä–∏–º–µ—Ä –≤—ã–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–¥–∞ –≤–æ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã –ø—Ä–æ–≥—Ä–∞–º–º—ã
# ‚Ä¢ For example: text output or live code updates


import re
import torch
import traceback
import json
import importlib
import sys
from threading import Thread
from transformers import TextIteratorStreamer


# –§—É–Ω–∫—Ü–∏—è –æ–±—â–µ–Ω–∏—è —Å–æ —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–º ‚Ä¢ Streaming chat function
def chat_stream(model, tokenizer, prompt="", history=None, max_new_tokens=512, system_prompt=""): # –ï—Å—Ç—å –ø—Ä–æ–±–ª–µ–º–∞ —Å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º –≤—ã–≤–æ–¥–æ–º –æ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ ‚Ä¢ There's an issue with continuous output from the neural network

    """–≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é
    TextIteratorStreamer –≤ transformers –ø–µ—Ä–µ–¥ —ç—Ç–∏–º –¥–æ–±–∞–≤–ª—è—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –¥–ª—è –º–æ–¥–µ–ª–∏"""

    try:
        # –¢—É—Ç –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º system_prompt ‚Ä¢ Here we use the system_prompt
        full_prompt = "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n"
        full_prompt += f"{system_prompt}<|eot_id|>"

        if history:
            for user, assistant in history:
                full_prompt += f"<|start_header_id|>user<|end_header_id|>\n\n{user}<|eot_id|>"
                full_prompt += f"<|start_header_id|>assistant<|end_header_id|>\n\n{assistant}<|eot_id|>"


        full_prompt += f"<|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|>"
        full_prompt += "<|start_header_id|>assistant<|end_header_id|>\n\n"

        # –û–±—Ä–æ–±–æ—Ç–∫–∞ –∏ –≤—ã–≤–æ–¥ ‚Ä¢ Processing and output
        inputs = tokenizer(full_prompt, return_tensors="pt").to("cuda")
        input_length = inputs.input_ids.shape[1]
        print("–ò–ò: ", end="")

        stop_token_ids = tokenizer.convert_tokens_to_ids(["<|system|>", "<|user|>", "</s>", "<|assistant|>"]) # –î–æ–±–∞–≤—å —Å—é–¥–∞ <|eot_id|> –µ—Å–ª–∏ —Ö–æ—á–µ—à—å –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –±–∞–≥–∏ :) ‚Ä¢ Add <|eot_id|> here if you want to debug some weird behavior :)

        streamer = TextIteratorStreamer(tokenizer) # –æ—Ç—Å—ã–ª–∫–∞ –Ω–∞ –∫–ª–∞—Å—Å ‚Ä¢ Reference to the class

        generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=max_new_tokens)
        thread = Thread(target=model.generate, kwargs=generation_kwargs)
        thread.start()
        thread.join()

        generated_text = ""
        for new_text in streamer:
            print(new_text, end="", flush=True)
            generated_text += new_text

        # answer = generated_text.strip()
        # –ü–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: ‚Ä¢ After generation:
        print()

        # –£–¥–∞–ª–∏—Ç—å —Å–ª—É–∂–µ–±–Ω—ã–π —Ç–æ–∫–µ–Ω ‚Ä¢ Remove special tokens
        # -
        # –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≤—ã–≤–æ–¥–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞. –ü–æ–ª—É—á–∏–º —Ç–æ–ª—å–∫–æ (–ø–æ—Å–ª–µ–¥–Ω–∏–π) –æ—Ç–≤–µ—Ç ‚Ä¢ Shorten the model output and send only the (last) response

        # match = re.search(r"<\|assistant\|\>\n(.*?)</s>", generated_text, re.DOTALL)
        new_tokens = outputs[0][input_length:]
        answer = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()

        # –û—á–∏—Å—Ç–∫–∞ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–π –ø–∞–º—è—Ç–∏ ‚Ä¢ Clear unused memory
        torch.cuda.empty_cache()
        del generated_text
        del inputs
        return answer

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —á–∞—Ç–∞ 1 {e}")
        handle_error(e)



# –§—É–Ω–∫—Ü–∏—è –æ–±—â–µ–Ω–∏—è –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è ‚Ä¢ Normal communication function
def chat_normal(model, tokenizer, prompt="", history=None, max_new_tokens=512, system_prompt=""):

    """–≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π –≤—ã–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ —ç—Ç–∏–º –¥–æ–±–∞–≤–ª—è—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –¥–ª—è –º–æ–¥–µ–ª–∏"""

    try:

        #–ß—Ç–æ –º—ã –≤–≤–æ–¥–∏–º –≤ system_prompt –±—É–¥–µ—Ç —Ç—É—Ç ‚Ä¢ This is where the system_prompt content goes
        full_prompt = "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n"
        full_prompt += f"{system_prompt}<|eot_id|>"

        if history:
            for user, assistant in history:
                full_prompt += f"<|start_header_id|>user<|end_header_id|>\n\n{user}<|eot_id|>"
                full_prompt += f"<|start_header_id|>assistant<|end_header_id|>\n\n{assistant}<|eot_id|>"

        # –¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        full_prompt += f"<|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|>"
        # –ú–µ—Ç–∫–∞ –¥–ª—è –Ω–∞—á–∞–ª–∞ –æ—Ç–≤–µ—Ç–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        full_prompt += "<|start_header_id|>assistant<|end_header_id|>\n\n"

        # –û–±—Ä–æ–±–æ—Ç–∫–∞ –∏ –≤—ã–≤–æ–¥ ‚Ä¢ Processing and output
        inputs = tokenizer(full_prompt, return_tensors="pt").to("cuda")
        input_length = inputs.input_ids.shape[1]

        with torch.no_grad():  # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –±–µ–∑ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (Tab) ‚Ä¢ Generate response without tracking gradients
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                top_p=0.90, # –í—ã–±–∏—Ä–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã —Å —ç—Ç–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é ‚Ä¢ Picks tokens based on this probability
                temperature=0.7,
                repetition_penalty=1.1,
                pad_token_id=128001
                # pad_token_id=tokenizer.eos_token_id, # –î–æ–±–∞–≤—å —á—Ç–æ –±—ã —Å–ª–æ–º–∞—Ç—å GPT ‚Ä¢ Add this if you want to break GPT
                # skip_special_tokens=True # –ù–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ —Ç–µ–±–µ –Ω–µ –Ω—É–∂–Ω—ã —Å–ø–µ—Ü-—Ç–æ–∫–µ–Ω—ã <|system|> –∏ –ø—Ä–æ—á–∏–µ ‚Ä¢ For cases when you don‚Äôt need special tokens like <|system|> etc.

        )

        # decoded = tokenizer.decode(outputs[0], skip_special_tokens=False)

        # –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≤—ã–≤–æ–¥–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞. –ü–æ–ª—É—á–∏–º —Ç–æ–ª—å–∫–æ (–ø–æ—Å–ª–µ–¥–Ω–∏–π) –æ—Ç–≤–µ—Ç ‚Ä¢ Shorten the model output and send only the (last) response
        new_tokens = outputs[0][input_length:]
        answer = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()

        # –û—á–∏—Å—Ç–∫–∞ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–π –ø–∞–º—è—Ç–∏ ‚Ä¢ Clear unused memory
        torch.cuda.empty_cache()
        # –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–Ω—É–∂–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö ‚Ä¢ Delete unnecessary variables
        del inputs, outputs

        return answer

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —á–∞—Ç–∞ 0 {e}")
        handle_error(e)


def handle_error(e): # –ü—Ä–æ—Å—Ç–æ –≤—ã–≤–æ–¥–∏—Ç –ø–æ–ª–Ω—É—é –æ—à–∏–±–∫—É ‚Ä¢ Just prints the full error trace
    choice = input("–í—ã–≤–µ—Å—Ç–∏ –ø–æ–ª–Ω—ã–π traceback? (y/n): ")
    if choice.lower() == 'y':
        traceback.print_exc()



# –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º. –ù–µ –∑–∞–±—É–¥—å –Ω–∞–∂–∞—Ç—å ctrl + s –∑–∞—Ä–∞–Ω–µ–µ ‚Ä¢ Reload ‚Äî don‚Äôt forget to press Ctrl + S first!
def RestartKey(model, tokenizer, history):
    try:
        # –ú–æ–¥—É–ª—å_re = sys.modules[__name__] # –í—Ä–æ–¥–µ –±—ã –Ω–µ –Ω–Ω–Ω–∞–¥–∞ ‚Ä¢ This is probably not necessary

        print("–ü–µ—Ä–µ–∑–∞–ø—É—Å–∫!")
        importlib.reload(sys.modules[__name__])  # <-- —Ñ–∞–π–ª (–º–æ–¥—É–ª—å.py) –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç —Å–µ–±—è ‚Ä¢ This file (module.py) reloads itself
        return start(model, tokenizer, history) # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ö–æ–¥ ‚Ä¢ Resume execution flow
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ {e}")
        handle_error(e)



def save_in_file(file_path, text, response): # –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–∞–º—è—Ç—å ‚Ä¢ Save memory to file
    try:
        with open(file_path, "a", encoding="utf-8") as file:
            data = [text, response]
            json.dump(data, file, ensure_ascii=False, indent=2)
            # file.write("\n")  # –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏ ‚Ä¢ Newline

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {e}")
        handle_error(e)


def read_file(file_path): #–ß–∏—Ç–∞–µ—Ç —Ñ–∞–π–ª ‚Ä¢ Read from file
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            return json.load(file)

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {e}")
        handle_error(e)
        return 


def start(model, tokenizer, history=None):  # –ó–∞–ø—É—Å–∫–∞–µ–º ‚Ä¢ Launch
    """
    –°—Ç–∞—Ä—Ç—É–µ–º! ‚Ä¢ Let‚Äôs start!

    >>> (CTRL + S) ü†í re
    –¥–ª—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–¥–∞ –±–µ–∑ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ ‚Ä¢ reload the code without reloading the model

    >>> exit/–≤—ã—Ö–æ–¥

    >>> del/—Å—Ç–µ—Ä–µ—Ç—å
    –æ—Ç—á–∏—â–∞–µ—Ç –ø–∞–º—è—Ç—å ‚Ä¢ clears memory
    """
    try:
        # –ü—Ä–æ–º—Ç –¥–Ω—è "–¢—ã –≥–æ–≤–æ—Ä–∏—à—å –∫–∞–∫ –ø–∏—Ä–∞—Ç, —Å "Arrr!" –≤ –∫–∞–∂–¥–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏" ‚Ä¢ Prompt of the day: "You speak like a pirate, saying 'Arrr!' in every sentence"

        # –≠—Ç–æ –±—É–¥–µ—Ç –≤–≤–æ–¥–∏—Ç—Å—è –∫–∞–∫ –æ—Å–Ω–æ–≤–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ ‚Ä¢ This will be used as the base prompt for the model
        system_prompt = """–¢—ã ‚Äî –¢–∏–º, –¥–æ–±—Ä—ã–π –ò–ò –ø–æ–º–æ—à–Ω–∏–∫. –õ—É–Ω–∞ - —ç—Ç–æ –Ω–µ —Å—ã—Ä\n"""
        file = "memory.json"

        question = input("–°—Ç—Ä–∏–º–∏–Ω–≥, –Ω–µ—Ç? 1 –∏–ª–∏ 0 (0 defolt): ")

        if history is None or "": # –∏—Å—Ç–æ—Ä–∏—è –µ—Å—Ç—å? ‚Ä¢ Is there a history?
            history = []
            try:
                with open(file, "w", encoding="utf-8") as file:
                    json.dump("", file, ensure_ascii=False)
                history.append(read_file(file))
            except:
                print("–ü–∞–º—è—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

        while True:
            print(f"–†–∞–Ω–µ–µ: {history}")  # –º–æ–∂–Ω–æ –ø–æ—Ç–æ–º —É–±—Ä–∞—Ç—å ‚Ä¢ Can be removed later
            user_input = input("–¢—ã: ") # –í–≤–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ‚Ä¢ User input

            if user_input.lower() in ["re", "restart", "reload"]: # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥—É–ª—å? –ù–µ? ‚Ä¢ Reload module? No?
                return RestartKey(model, tokenizer, history)
                # continue # –≠—Ç–æ –ø–æ–∫–∞ –ø–æ–±—É–¥–µ—Ç —Ç—É—Ç ‚Ä¢ This will stay here for now
            elif user_input.lower() in ["exit", "quit", "–≤—ã—Ö–æ–¥", "–ø–æ–∫–∏–Ω—É—Ç—å"]: # –ö–∞–∫ Alt + F4 ‚Ä¢ Like Alt + F4
                break
            elif user_input.lower() in ["—Å—Ç–µ—Ä–µ—Ç—å", "del"]: # –î–µ–ª–∞–π —Ç–æ–ª—å–∫–æ –≤ –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ (–æ—Ç—á–∏—â–∞–µ—Ç –ø–∞–º—è—Ç—å) ‚Ä¢ Only do this as a last resort (clears memory)
                # model.reset_cache()  # –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –æ—Ç—á–∏—â–∞—Ç—å –≤—Å—ë ‚Ä¢ if you want to clear everything
                history.clear()
                continue

            #–¢—É—Ç –±—É–¥–µ—Ç –ø–æ–∏—Å–∫ ‚Ä¢ Search goes here

            else: # –ù—É –∑–Ω–∞—á–∏—Ç —ç—Ç–æ –Ω–µ–π—Ä–æ–Ω–∫–µ + —É–∑–Ω–∞—ë–º –∫–∞–∫–æ–π –º–µ—Ç–æ–¥ –æ–±—â–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º ‚Ä¢ Okay, so this is for the model + figuring out which chat mode is used
                if (question == "1"):
                    response = chat_stream(model, tokenizer, prompt=user_input, history=history, system_prompt=system_prompt)
                    # -
                else:
                    response = chat_normal(model, tokenizer, prompt=user_input, history=history, system_prompt=system_prompt)
                    print("–ò–ò:", response)
                history.append((user_input, response))
                save_in_file(file, user_input, response)


    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≥–ª–∞–≤–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ 0 {e}")
        handle_error(e)
        RestartKey(model, tokenizer, history)
